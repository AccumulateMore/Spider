{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫的BS4方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Beautiful Soup基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 对于一个网页来说，都有一定的特殊结构和层级关系，而且很多节点都有id或class来作区分，Beautiful Soup 就是借助网页的结构和属性等特性来解析网页，Beautiful Soup是一个强大的解析工具。\n",
    "\n",
    "② Beautiful Soup(简称BS4)提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。\n",
    "\n",
    "③ Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。\n",
    "\n",
    "④ Beautiful Soup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。\n",
    "\n",
    "⑤ Beautiful Soup不需要考虑编码方式，除非文档没有指定，重新原始编码方式即可。\n",
    "\n",
    "⑥ BeautifulSoup()第一个参数是字符串，即传给BeautiflSoup对象，第二个参数为解析器类型(这里使用xlml)，此时完成BeautifulSoup对象的初始化。然后，将这个对象赋值给soup变量，接下来，就可以调用soup的各个方法和属性解析这串HTML代码了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BS4获取节点名字和节点内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Hello</p>\n",
      "<class 'bs4.element.Tag'>\n",
      "Hello\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup('<p>Hello</p>','lxml')\n",
    "print(soup.p)\n",
    "print(type(soup.p))    # 类型是标签\n",
    "print(soup.p.string)   # 获取节点中内容\n",
    "print(soup.p.name)     # 获取节点名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>duanluo1</p>\n",
      "<class 'bs4.element.Tag'>\n",
      "duanluo1\n",
      "p\n",
      "{'id': '1'}\n",
      "1\n",
      "<h1>biaoti1</h1>\n"
     ]
    }
   ],
   "source": [
    "html_str = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset = \"utf-8\">\n",
    "<title>(runoob.com)</title>    \n",
    "</head>  \n",
    "<body>\n",
    "<div id = 1>\n",
    " <div id = 2>\n",
    "  <h1>biaoti1</h1>\n",
    "  <h2>biaoti2</h2>\n",
    "  <h3>biaoti3</h3>\n",
    "  <p>duanluo1</p>\n",
    " </div>\n",
    " <br>\n",
    " <br>\n",
    " <br>\n",
    " <p id=1>duanluo2</p>\n",
    " <p id=2>duanluo3</p>\n",
    " <p id=3>duanluo4</p>\n",
    " <p>duanluo5</p>\n",
    " <a href='//www.runoob.com'>link</a>\n",
    " <h1>biaoti4</h1>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_str,'lxml')\n",
    "print(soup.p)\n",
    "print(type(soup.p))    # 类型是标签\n",
    "print(soup.p.string)   # 获取节点内容\n",
    "print(soup.p.name)     # 获取节点名字\n",
    "print(soup.div.attrs)  # 获取节点标签所属信息\n",
    "print(soup.div.attrs['id'])\n",
    "# 每个节点可能有多个属性，比如id和class等，选择这个节点元素后，可以调用attrs获取所有属性      \n",
    "\n",
    "# 方法一：\n",
    "# print(soup.html.contents)  # 打印内容\n",
    "\n",
    "'''\n",
    "# 方法二：\n",
    "print(soup.html.children)  # 生成一个迭代器\n",
    "for child in soup.html.children:    # 打印内容，等价于 print(soup.html.contents)\n",
    "    print(child)\n",
    "'''\n",
    "\n",
    "print(soup.body.div.div.h1)   # 原来用 \\，现在用.进入下一层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BS4获取父节中内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div id=\"2\">\n",
      "<h1>biaoti1</h1>\n",
      "<h2>biaoti2</h2>\n",
      "<h3>biaoti3</h3>\n",
      "<p>duanluo1</p>\n",
      "</div>, '\\n']\n",
      "<div id=\"2\">\n",
      "<h1>biaoti1</h1>\n",
      "<h2>biaoti2</h2>\n",
      "<h3>biaoti3</h3>\n",
      "<p>duanluo1</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "html_str = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset = \"utf-8\">\n",
    "<title>(runoob.com)</title    \n",
    "</head>  \n",
    "<body>\n",
    "<div id = 1>\n",
    " <div id = 2>\n",
    "  <h1>biaoti1</h1>\n",
    "  <h2>biaoti2</h2>\n",
    "  <h3>biaoti3</h3>\n",
    "  <p>duanluo1</p>\n",
    " </div><br>\n",
    " <br>\n",
    " <br>\n",
    " <p id=1>duanluo2</p>\n",
    " <p id=2>duanluo3</p>\n",
    " <p id=3>duanluo4</p>\n",
    " <p>duanluo5</p>\n",
    " <a href='//www.runoob.com'>link</a>\n",
    " <h1>biaoti4</h1>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_str,'lxml')\n",
    "print(list(soup.br.previous_siblings)) # 上面的节点,把换行符也提出来了\n",
    "print(soup.br.previous_sibling)        # 这里</div><br>为一行，如果为两行，就把前一个的换行符打印出来了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BS4获取筛选后内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 find_all筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "\n",
      "\n",
      "[<p id=\"2\">duanluo3</p>]\n",
      "\n",
      "\n",
      "[<div class=\"a\" id=\"1\">\n",
      "<div class=\"b\" id=\"2\">\n",
      "<h1>biaoti1</h1>\n",
      "<h2>biaoti2</h2>\n",
      "<h3>biaoti3</h3>\n",
      "<p>duanluo1</p>\n",
      "</div> <br/>\n",
      "<br/>\n",
      "<br/>\n",
      "<p id=\"1\">duanluo2</p>\n",
      "<p id=\"2\">duanluo3</p>\n",
      "<p id=\"3\">duanluo4</p>\n",
      "<p>duanluo5</p>\n",
      "<a href=\"//www.runoob.com\">link</a>\n",
      "<h1>biaoti4</h1>\n",
      "</div>, <p id=\"1\">duanluo2</p>]\n",
      "\n",
      "\n",
      "[<div class=\"a\" id=\"1\">\n",
      "<div class=\"b\" id=\"2\">\n",
      "<h1>biaoti1</h1>\n",
      "<h2>biaoti2</h2>\n",
      "<h3>biaoti3</h3>\n",
      "<p>duanluo1</p>\n",
      "</div> <br/>\n",
      "<br/>\n",
      "<br/>\n",
      "<p id=\"1\">duanluo2</p>\n",
      "<p id=\"2\">duanluo3</p>\n",
      "<p id=\"3\">duanluo4</p>\n",
      "<p>duanluo5</p>\n",
      "<a href=\"//www.runoob.com\">link</a>\n",
      "<h1>biaoti4</h1>\n",
      "</div>]\n",
      "\n",
      "\n",
      "[<p>duanluo1</p>, <p id=\"1\">duanluo2</p>, <p id=\"2\">duanluo3</p>, <p id=\"3\">duanluo4</p>, <p>duanluo5</p>]\n",
      "[<p>duanluo1</p>]\n"
     ]
    }
   ],
   "source": [
    "html_str = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset = \"utf-8\">\n",
    "<title>(runoob.com)</title    \n",
    "</head>  \n",
    "<body>\n",
    "<div id = 1 class='a'>\n",
    " <div id = 2 class='b'>\n",
    "  <h1>biaoti1</h1>\n",
    "  <h2>biaoti2</h2>\n",
    "  <h3>biaoti3</h3>\n",
    "  <p>duanluo1</p>\n",
    " </div> <br>                \n",
    " <br>\n",
    " <br>\n",
    " <p id=1>duanluo2</p>\n",
    " <p id=2>duanluo3</p>\n",
    " <p id=3>duanluo4</p>\n",
    " <p>duanluo5</p>\n",
    " <a href='//www.runoob.com'>link</a>\n",
    " <h1>biaoti4</h1>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "soup = BeautifulSoup(html_str,'lxml')\n",
    "print(type(soup.find_all(name='p')[0]))\n",
    "print(\"\\n\")\n",
    "print(soup.find_all(name='p',attrs={\"id\":2}))   # 查询条件，通过标签筛选\n",
    "print(\"\\n\")\n",
    "print(soup.find_all(id=1))     \n",
    "print(\"\\n\")\n",
    "print(soup.find_all(class_='a'))  # class是关键字，因此提取的时候要加一个下划线\n",
    "print(\"\\n\")\n",
    "print(soup.find_all(name='p'))\n",
    "print(soup.find_all(name='p',text=re.compile('.*?1')))   # 把文本中有1的筛选出来       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 select筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "\n",
      "        XGBoost类库使用小结\n",
      "     \n",
      "摘要：在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档 和 XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth        阅读全文\n",
      " posted @ 2019-07-01 18:10\n",
      "刘建平Pinard\n",
      "阅读(30645)\n",
      "评论(115)\n",
      "推荐(14)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(30645) 评论(115) 推荐(14)\n",
      "\n",
      "        XGBoost算法原理小结\n",
      "     \n",
      "摘要：在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P        阅读全文\n",
      " posted @ 2019-06-05 20:36\n",
      "刘建平Pinard\n",
      "阅读(35409)\n",
      "评论(181)\n",
      "推荐(18)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(35409) 评论(181) 推荐(18)\n",
      "\n",
      "        机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导\n",
      "     \n",
      "摘要：在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章        阅读全文\n",
      " posted @ 2019-05-27 17:19\n",
      "刘建平Pinard\n",
      "阅读(19180)\n",
      "评论(16)\n",
      "推荐(6)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(19180) 评论(16) 推荐(6)\n",
      "\n",
      "        机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则\n",
      "     \n",
      "摘要：在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果        阅读全文\n",
      " posted @ 2019-05-07 15:59\n",
      "刘建平Pinard\n",
      "阅读(27098)\n",
      "评论(62)\n",
      "推荐(16)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(27098) 评论(62) 推荐(16)\n",
      "\n",
      "        机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法\n",
      "     \n",
      "摘要：在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量        阅读全文\n",
      " posted @ 2019-04-29 19:42\n",
      "刘建平Pinard\n",
      "阅读(21109)\n",
      "评论(78)\n",
      "推荐(13)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(21109) 评论(78) 推荐(13)\n",
      "\n",
      "        机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法\n",
      "     \n",
      "摘要：在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导        阅读全文\n",
      " posted @ 2019-04-26 18:42\n",
      "刘建平Pinard\n",
      "阅读(21724)\n",
      "评论(36)\n",
      "推荐(10)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(21724) 评论(36) 推荐(10)\n",
      "\n",
      "        机器学习中的矩阵向量求导(一) 求导定义与求导布局\n",
      "     \n",
      "摘要：在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的        阅读全文\n",
      " posted @ 2019-04-22 18:03\n",
      "刘建平Pinard\n",
      "阅读(33216)\n",
      "评论(19)\n",
      "推荐(31)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(33216) 评论(19) 推荐(31)\n",
      "\n",
      "        强化学习(十九) AlphaGo Zero强化学习原理\n",
      "     \n",
      "摘要：在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG        阅读全文\n",
      " posted @ 2019-03-27 20:11\n",
      "刘建平Pinard\n",
      "阅读(20559)\n",
      "评论(63)\n",
      "推荐(8)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(20559) 评论(63) 推荐(8)\n",
      "\n",
      "        强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)\n",
      "     \n",
      "摘要：在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参        阅读全文\n",
      " posted @ 2019-03-04 17:09\n",
      "刘建平Pinard\n",
      "阅读(26724)\n",
      "评论(22)\n",
      "推荐(3)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(26724) 评论(22) 推荐(3)\n",
      "\n",
      "        强化学习(十七) 基于模型的强化学习与Dyna算法框架\n",
      "     \n",
      "摘要：在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy        阅读全文\n",
      " posted @ 2019-02-15 20:22\n",
      "刘建平Pinard\n",
      "阅读(12176)\n",
      "评论(24)\n",
      "推荐(2)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(12176) 评论(24) 推荐(2)\n",
      "\n",
      "        强化学习(十六) 深度确定性策略梯度(DDPG)\n",
      "     \n",
      "摘要：在强化学习(十五) A3C中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Poli        阅读全文\n",
      " posted @ 2019-02-01 19:42\n",
      "刘建平Pinard\n",
      "阅读(53589)\n",
      "评论(270)\n",
      "推荐(10)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(53589) 评论(270) 推荐(10)\n",
      "\n",
      "        强化学习(十五) A3C\n",
      "     \n",
      "摘要：在强化学习(十四) Actor-Critic中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C        阅读全文\n",
      " posted @ 2019-01-29 18:09\n",
      "刘建平Pinard\n",
      "阅读(35682)\n",
      "评论(97)\n",
      "推荐(3)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(35682) 评论(97) 推荐(3)\n",
      "\n",
      "        强化学习(十四) Actor-Critic\n",
      "     \n",
      "摘要：在强化学习(十三) 策略梯度(Policy Gradient)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。 在本篇我们讨论策略(Policy        阅读全文\n",
      " posted @ 2019-01-15 17:46\n",
      "刘建平Pinard\n",
      "阅读(47074)\n",
      "评论(130)\n",
      "推荐(3)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(47074) 评论(130) 推荐(3)\n",
      "\n",
      "        强化学习(十三) 策略梯度(Policy Gradient)\n",
      "     \n",
      "摘要：在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradien        阅读全文\n",
      " posted @ 2018-12-18 18:04\n",
      "刘建平Pinard\n",
      "阅读(62843)\n",
      "评论(147)\n",
      "推荐(8)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(62843) 评论(147) 推荐(8)\n",
      "\n",
      "        强化学习(十二) Dueling DQN\n",
      "     \n",
      "摘要：在强化学习(十一) Prioritized Replay DQN中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的deep RL tutorial和Dueling DQN的论文<Dueling N        阅读全文\n",
      " posted @ 2018-11-08 14:04\n",
      "刘建平Pinard\n",
      "阅读(27524)\n",
      "评论(52)\n",
      "推荐(4)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(27524) 评论(52) 推荐(4)\n",
      "\n",
      "        强化学习(十一) Prioritized Replay DQN\n",
      "     \n",
      "摘要：在强化学习（十）Double DQN (DDQN)中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay         阅读全文\n",
      " posted @ 2018-10-16 16:46\n",
      "刘建平Pinard\n",
      "阅读(26534)\n",
      "评论(106)\n",
      "推荐(9)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(26534) 评论(106) 推荐(9)\n",
      "\n",
      "        强化学习（十）Double DQN (DDQN)\n",
      "     \n",
      "摘要：在强化学习（九）Deep Q-Learning进阶之Nature DQN中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称D        阅读全文\n",
      " posted @ 2018-10-12 16:52\n",
      "刘建平Pinard\n",
      "阅读(50409)\n",
      "评论(45)\n",
      "推荐(7)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(50409) 评论(45) 推荐(7)\n",
      "\n",
      "        强化学习（九）Deep Q-Learning进阶之Nature DQN\n",
      "     \n",
      "摘要：在强化学习（八）价值函数的近似表示与Deep Q-Learning中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 201        阅读全文\n",
      " posted @ 2018-10-08 20:40\n",
      "刘建平Pinard\n",
      "阅读(36337)\n",
      "评论(76)\n",
      "推荐(6)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(36337) 评论(76) 推荐(6)\n",
      "\n",
      "        强化学习（八）价值函数的近似表示与Deep Q-Learning\n",
      "     \n",
      "摘要：在强化学习系列的前七篇里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。 Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。 1. 为何需要价值函数        阅读全文\n",
      " posted @ 2018-09-28 16:49\n",
      "刘建平Pinard\n",
      "阅读(45218)\n",
      "评论(179)\n",
      "推荐(7)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(45218) 评论(179) 推荐(7)\n",
      "\n",
      "        强化学习（七）时序差分离线控制算法Q-Learning\n",
      "     \n",
      "摘要：在强化学习（六）时序差分在线控制算法SARSA中我们讨论了时序差分的在线控制算法SARSA，而另一类时序差分的离线控制算法还没有讨论，因此本文我们关注于时序差分离线控制算法，主要是经典的Q-Learning算法。 Q-Learning这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部        阅读全文\n",
      " posted @ 2018-09-19 19:32\n",
      "刘建平Pinard\n",
      "阅读(31347)\n",
      "评论(105)\n",
      "推荐(9)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(31347) 评论(105) 推荐(9)\n",
      "\n",
      "        强化学习（六）时序差分在线控制算法SARSA\n",
      "     \n",
      "摘要：在强化学习（五）用时序差分法（TD）求解中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。 SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。 1. SARSA算法的引入 S        阅读全文\n",
      " posted @ 2018-09-09 19:30\n",
      "刘建平Pinard\n",
      "阅读(30330)\n",
      "评论(65)\n",
      "推荐(9)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(30330) 评论(65) 推荐(9)\n",
      "\n",
      "        强化学习（五）用时序差分法（TD）求解\n",
      "     \n",
      "摘要：在强化学习（四）用蒙特卡罗法（MC）求解中，我们讲到了使用蒙特卡罗法来求解强化学习问题的方法，虽然蒙特卡罗法很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果我们没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。本文我们就来讨论可以不使用完整状态序列求解强化        阅读全文\n",
      " posted @ 2018-08-24 18:23\n",
      "刘建平Pinard\n",
      "阅读(39293)\n",
      "评论(125)\n",
      "推荐(8)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(39293) 评论(125) 推荐(8)\n",
      "\n",
      "        强化学习（四）用蒙特卡罗法（MC）求解\n",
      "     \n",
      "摘要：在强化学习（三）用动态规划（DP）求解中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型$P$都无法知道，这时动态规划法根本没法        阅读全文\n",
      " posted @ 2018-08-17 18:04\n",
      "刘建平Pinard\n",
      "阅读(39026)\n",
      "评论(96)\n",
      "推荐(11)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(39026) 评论(96) 推荐(11)\n",
      "\n",
      "        强化学习（三）用动态规划（DP）求解\n",
      "     \n",
      "摘要：在强化学习（二）马尔科夫决策过程(MDP)中，我们讨论了用马尔科夫假设来简化强化学习模型的复杂度，这一篇我们在马尔科夫假设和贝尔曼方程的基础上讨论使用动态规划(Dynamic Programming, DP)来求解强化学习的问题。 动态规划这一篇对应Sutton书的第四章和UCL强化学习课程的第三讲        阅读全文\n",
      " posted @ 2018-08-12 20:36\n",
      "刘建平Pinard\n",
      "阅读(37043)\n",
      "评论(80)\n",
      "推荐(11)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(37043) 评论(80) 推荐(11)\n",
      "\n",
      "        强化学习（二）马尔科夫决策过程(MDP)\n",
      "     \n",
      "摘要：在强化学习（一）模型基础中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。 MDP        阅读全文\n",
      " posted @ 2018-08-05 18:09\n",
      "刘建平Pinard\n",
      "阅读(82748)\n",
      "评论(111)\n",
      "推荐(16)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(82748) 评论(111) 推荐(16)\n",
      "\n",
      "        强化学习（一）模型基础\n",
      "     \n",
      "摘要：从今天开始整理强化学习领域的知识，主要参考的资料是Sutton的强化学习书和UCL强化学习的课程。这个系列大概准备写10到20篇，希望写完后自己的强化学习碎片化知识可以得到融会贯通，也希望可以帮到更多的人，毕竟目前系统的讲解强化学习的中文资料不太多。 第一篇会从强化学习的基本概念讲起，对应Sutto        阅读全文\n",
      " posted @ 2018-07-29 18:53\n",
      "刘建平Pinard\n",
      "阅读(83228)\n",
      "评论(58)\n",
      "推荐(22)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(83228) 评论(58) 推荐(22)\n",
      "\n",
      "        异常点检测算法小结\n",
      "     \n",
      "摘要：异常点检测，有时也叫离群点检测，英文一般叫做Novelty Detection或者Outlier Detection,是比较常见的一类非监督学习算法，这里就对异常点检测算法做一个总结。 1. 异常点检测算法使用场景 什么时候我们需要异常点检测算法呢？常见的有三种情况。一是在做特征工程的时候需要对异常        阅读全文\n",
      " posted @ 2018-07-15 19:19\n",
      "刘建平Pinard\n",
      "阅读(38668)\n",
      "评论(74)\n",
      "推荐(9)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(38668) 评论(74) 推荐(9)\n",
      "\n",
      "        tensorflow机器学习模型的跨平台上线\n",
      "     \n",
      "摘要：在用PMML实现机器学习模型的跨平台上线中，我们讨论了使用PMML文件来实现跨平台模型上线的方法，这个方法当然也适用于tensorflow生成的模型，但是由于tensorflow模型往往较大，使用无法优化的PMML文件大多数时候很笨拙，因此本文我们专门讨论下tensorflow机器学习模型的跨平台上        阅读全文\n",
      " posted @ 2018-07-01 21:42\n",
      "刘建平Pinard\n",
      "阅读(10974)\n",
      "评论(16)\n",
      "推荐(2)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(10974) 评论(16) 推荐(2)\n",
      "\n",
      "        用PMML实现机器学习模型的跨平台上线\n",
      "     \n",
      "摘要：在机器学习用于产品的时候，我们经常会遇到跨平台的问题。比如我们用Python基于一系列的机器学习库训练了一个模型，但是有时候其他的产品和项目想把这个模型集成进去，但是这些产品很多只支持某些特定的生产环境比如Java，为了上一个机器学习模型去大动干戈修改环境配置很不划算，此时我们就可以考虑用预测模型标        阅读全文\n",
      " posted @ 2018-06-24 15:18\n",
      "刘建平Pinard\n",
      "阅读(33238)\n",
      "评论(67)\n",
      "推荐(12)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(33238) 评论(67) 推荐(12)\n",
      "\n",
      "        用tensorflow学习贝叶斯个性化排序(BPR)\n",
      "     \n",
      "摘要：在贝叶斯个性化排序(BPR)算法小结中，我们对贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)的原理做了讨论，本文我们将从实践的角度来使用BPR做一个简单的推荐。由于现有主流开源类库都没有BPR，同时它又比较简单，因此用tensorflow自己实现一个        阅读全文\n",
      " posted @ 2018-06-10 17:29\n",
      "刘建平Pinard\n",
      "阅读(14575)\n",
      "评论(46)\n",
      "推荐(5)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(14575) 评论(46) 推荐(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        贝叶斯个性化排序(BPR)算法小结\n",
      "     \n",
      "摘要：在矩阵分解在协同过滤推荐算法中的应用中，我们讨论过像funkSVD之类的矩阵分解方法如何用于推荐。今天我们讲另一种在实际产品中用的比较多的推荐算法:贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)，它也用到了矩阵分解，但是和funkSVD家族却有很多不        阅读全文\n",
      " posted @ 2018-06-03 16:22\n",
      "刘建平Pinard\n",
      "阅读(28640)\n",
      "评论(40)\n",
      "推荐(10)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(28640) 评论(40) 推荐(10)\n",
      "\n",
      "        特征工程之特征预处理\n",
      "     \n",
      "摘要：在前面我们分别讨论了特征工程中的特征选择与特征表达，本文我们来讨论特征预处理的相关问题。主要包括特征的归一化和标准化，异常特征样本清洗与样本数据不平衡问题的处理。 1. 特征的标准化和归一化 由于标准化和归一化这两个词经常混用，所以本文不再区别标准化和归一化，而通过具体的标准化和归一化方法来区别具体        阅读全文\n",
      " posted @ 2018-05-26 20:23\n",
      "刘建平Pinard\n",
      "阅读(21223)\n",
      "评论(106)\n",
      "推荐(23)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(21223) 评论(106) 推荐(23)\n",
      "\n",
      "        特征工程之特征表达\n",
      "     \n",
      "摘要：在特征工程之特征选择中，我们讲到了特征选择的一些要点。本篇我们继续讨论特征工程，不过会重点关注于特征表达部分，即如果对某一个特征的具体表现形式做处理。主要包括缺失值处理，特殊的特征处理比如时间和地理位置处理，离散特征的连续化和离散化处理，连续特征的离散化处理几个方面。 1. 缺失值处理 特征有缺失值        阅读全文\n",
      " posted @ 2018-05-19 22:39\n",
      "刘建平Pinard\n",
      "阅读(21563)\n",
      "评论(100)\n",
      "推荐(11)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(21563) 评论(100) 推荐(11)\n",
      "\n",
      "        特征工程之特征选择\n",
      "     \n",
      "摘要：特征工程是数据分析中最耗时间和精力的一部分工作，它不像算法和模型那样是确定的步骤，更多是工程上的经验和权衡。因此没有统一的方法。这里只是对一些常用的方法做一个总结。本文关注于特征选择部分。后面还有两篇会关注于特征表达和特征预处理。 1. 特征的来源 在做数据分析的时候，特征的来源一般有两块，一块是业        阅读全文\n",
      " posted @ 2018-05-13 20:13\n",
      "刘建平Pinard\n",
      "阅读(42715)\n",
      "评论(105)\n",
      "推荐(26)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(42715) 评论(105) 推荐(26)\n",
      "\n",
      "        用gensim学习word2vec\n",
      "     \n",
      "摘要：在word2vec原理篇中，我们对word2vec的两种模型CBOW和Skip-Gram，以及两种解法Hierarchical Softmax和Negative Sampling做了总结。这里我们就从实践的角度，使用gensim来学习word2vec。 1. gensim安装与概述 gensim是一        阅读全文\n",
      " posted @ 2017-08-03 14:12\n",
      "刘建平Pinard\n",
      "阅读(77546)\n",
      "评论(89)\n",
      "推荐(18)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(77546) 评论(89) 推荐(18)\n",
      "\n",
      "        word2vec原理(三) 基于Negative Sampling的模型\n",
      "     \n",
      "摘要：word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 在上一篇中我们讲到了基于Hierarchical Softmax的word2ve        阅读全文\n",
      " posted @ 2017-07-28 15:56\n",
      "刘建平Pinard\n",
      "阅读(80259)\n",
      "评论(120)\n",
      "推荐(23)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(80259) 评论(120) 推荐(23)\n",
      "\n",
      "        word2vec原理(二) 基于Hierarchical Softmax的模型\n",
      "     \n",
      "摘要：word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 在word2vec原理(一) CBOW与Skip-Gram模型基础中，我们讲到了        阅读全文\n",
      " posted @ 2017-07-27 17:26\n",
      "刘建平Pinard\n",
      "阅读(109837)\n",
      "评论(278)\n",
      "推荐(36)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(109837) 评论(278) 推荐(36)\n",
      "\n",
      "        word2vec原理(一) CBOW与Skip-Gram模型基础\n",
      "     \n",
      "摘要：word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 word2vec是google在2013年推出的一个NLP工具，它的特点是将所有        阅读全文\n",
      " posted @ 2017-07-13 16:34\n",
      "刘建平Pinard\n",
      "阅读(214917)\n",
      "评论(109)\n",
      "推荐(33)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(214917) 评论(109) 推荐(33)\n",
      "\n",
      "        条件随机场CRF(三) 模型学习与维特比算法解码\n",
      "     \n",
      "摘要：条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 在CRF系列的前两篇，我们总结了CRF的模型基础与第一个问题的求解方法，本文我们关注于linear-CRF的第二个问题与第三个问题的求解。第二个问        阅读全文\n",
      " posted @ 2017-06-23 15:10\n",
      "刘建平Pinard\n",
      "阅读(20681)\n",
      "评论(61)\n",
      "推荐(5)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(20681) 评论(61) 推荐(5)\n",
      "\n",
      "        条件随机场CRF(二) 前向后向算法评估标记序列概率\n",
      "     \n",
      "摘要：条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 在条件随机场CRF(一)中我们总结了CRF的模型，主要是linear-CRF的模型原理。本文就继续讨论linear-CRF需要解决的三个问题：评估        阅读全文\n",
      " posted @ 2017-06-22 14:14\n",
      "刘建平Pinard\n",
      "阅读(16960)\n",
      "评论(70)\n",
      "推荐(3)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(16960) 评论(70) 推荐(3)\n",
      "\n",
      "        条件随机场CRF(一)从随机场到线性链条件随机场\n",
      "     \n",
      "摘要：条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 条件随机场(Conditional Random Fields, 以下简称CRF)是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然        阅读全文\n",
      " posted @ 2017-06-19 17:32\n",
      "刘建平Pinard\n",
      "阅读(39341)\n",
      "评论(98)\n",
      "推荐(15)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(39341) 评论(98) 推荐(15)\n",
      "\n",
      "        用hmmlearn学习隐马尔科夫模型HMM\n",
      "     \n",
      "摘要：在之前的HMM系列中，我们对隐马尔科夫模型HMM的原理以及三个问题的求解方法做了总结。本文我们就从实践的角度用Python的hmmlearn库来学习HMM的使用。关于hmmlearn的更多资料在官方文档有介绍。 1. hmmlearn概述 hmmlearn安装很简单，\"pip install hmm        阅读全文\n",
      " posted @ 2017-06-13 16:24\n",
      "刘建平Pinard\n",
      "阅读(41852)\n",
      "评论(158)\n",
      "推荐(13)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(41852) 评论(158) 推荐(13)\n",
      "\n",
      "        隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列\n",
      "     \n",
      "摘要：隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在本篇我们会讨论HMM模型最后一个问题的求解，即即给定模型和观测序列，求给定观测序列条件下，最        阅读全文\n",
      " posted @ 2017-06-12 16:57\n",
      "刘建平Pinard\n",
      "阅读(28673)\n",
      "评论(35)\n",
      "推荐(6)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(28673) 评论(35) 推荐(6)\n",
      "\n",
      "        隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数\n",
      "     \n",
      "摘要：隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在本篇我们会讨论HMM模型参数求解的问题，这个问题在HMM三个问题里算是最复杂的。在研究这个问        阅读全文\n",
      " posted @ 2017-06-10 21:25\n",
      "刘建平Pinard\n",
      "阅读(28068)\n",
      "评论(97)\n",
      "推荐(12)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(28068) 评论(97) 推荐(12)\n",
      "\n",
      "        隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率\n",
      "     \n",
      "摘要：隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在隐马尔科夫模型HMM（一）HMM模型中，我们讲到了HMM模型的基础知识和HMM的三个基本问题        阅读全文\n",
      " posted @ 2017-06-08 08:47\n",
      "刘建平Pinard\n",
      "阅读(43772)\n",
      "评论(56)\n",
      "推荐(19)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(43772) 评论(56) 推荐(19)\n",
      "\n",
      "        隐马尔科夫模型HMM（一）HMM模型\n",
      "     \n",
      "摘要：隐马尔科夫模型HMM（一）HMM模型基础 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学        阅读全文\n",
      " posted @ 2017-06-06 15:01\n",
      "刘建平Pinard\n",
      "阅读(88127)\n",
      "评论(37)\n",
      "推荐(26)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(88127) 评论(37) 推荐(26)\n",
      "\n",
      "        EM算法原理总结\n",
      "     \n",
      "摘要：EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。本文就对EM算法的原理做一个总结。 1. EM算法要解决的问题 我们经常会从样本观察数据中，找出样本的模型参        阅读全文\n",
      " posted @ 2017-05-27 17:12\n",
      "刘建平Pinard\n",
      "阅读(70613)\n",
      "评论(126)\n",
      "推荐(27)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(70613) 评论(126) 推荐(27)\n",
      "\n",
      "        用scikit-learn学习LDA主题模型\n",
      "     \n",
      "摘要：在LDA模型原理篇我们总结了LDA主题模型的原理，这里我们就从应用的角度来使用scikit-learn来学习LDA主题模型。除了scikit-learn, 还有spark MLlib和gensim库也有LDA主题模型的类库，使用的原理基本类似，本文关注于scikit-learn中LDA主题模型的使用        阅读全文\n",
      " posted @ 2017-05-26 15:23\n",
      "刘建平Pinard\n",
      "阅读(48959)\n",
      "评论(104)\n",
      "推荐(11)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(48959) 评论(104) 推荐(11)\n",
      "\n",
      "        文本主题模型之LDA(三) LDA求解之变分推断EM算法\n",
      "     \n",
      "摘要：文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 本文是LDA主题模型的第三篇，读这一篇之前建议先读文本主题模型之LDA(一) LDA基础，同时由于使用了EM算法，如果你对EM算法不熟悉，建议        阅读全文\n",
      " posted @ 2017-05-22 12:20\n",
      "刘建平Pinard\n",
      "阅读(23283)\n",
      "评论(100)\n",
      "推荐(10)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(23283) 评论(100) 推荐(10)\n",
      "\n",
      "        文本主题模型之LDA(二) LDA求解之Gibbs采样算法\n",
      "     \n",
      "摘要：文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 本文是LDA主题模型的第二篇，读这一篇之前建议先读文本主题模型之LDA(一) LDA基础，同时由于使用了基于MCMC的Gibbs采样算法，如果        阅读全文\n",
      " posted @ 2017-05-18 10:43\n",
      "刘建平Pinard\n",
      "阅读(42577)\n",
      "评论(216)\n",
      "推荐(7)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(42577) 评论(216) 推荐(7)\n",
      "\n",
      "        文本主题模型之LDA(一) LDA基础\n",
      "     \n",
      "摘要：文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 在前面我们讲到了基于矩阵分解的LSI和NMF主题模型，这里我们开始讨论被广泛使用的主题模型：隐含狄利克雷分布(Latent Dirichlet        阅读全文\n",
      " posted @ 2017-05-17 14:37\n",
      "刘建平Pinard\n",
      "阅读(157596)\n",
      "评论(105)\n",
      "推荐(21)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(157596) 评论(105) 推荐(21)\n",
      "\n",
      "        文本主题模型之非负矩阵分解(NMF)\n",
      "     \n",
      "摘要：在文本主题模型之潜在语义索引(LSI)中，我们讲到LSI主题模型使用了奇异值分解，面临着高维度计算量太大的问题。这里我们就介绍另一种基于矩阵分解的主题模型：非负矩阵分解(NMF)，它同样使用了矩阵分解，但是计算量和处理速度则比LSI快，它是怎么做到的呢？ 1. 非负矩阵分解(NMF)概述 非负矩阵分        阅读全文\n",
      " posted @ 2017-05-05 14:19\n",
      "刘建平Pinard\n",
      "阅读(21769)\n",
      "评论(15)\n",
      "推荐(8)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(21769) 评论(15) 推荐(8)\n",
      "\n",
      "        文本主题模型之潜在语义索引(LSI)\n",
      "     \n",
      "摘要：在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 1. 文本主题模型的问题特点 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型        阅读全文\n",
      " posted @ 2017-05-04 14:40\n",
      "刘建平Pinard\n",
      "阅读(29319)\n",
      "评论(48)\n",
      "推荐(10)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(29319) 评论(48) 推荐(10)\n",
      "\n",
      "        英文文本挖掘预处理流程总结\n",
      "     \n",
      "摘要：在中文文本挖掘预处理流程总结中，我们总结了中文文本挖掘的预处理流程，这里我们再对英文文本挖掘的预处理流程做一个总结。 1. 英文文本挖掘预处理特点 英文文本的预处理方法和中文的有部分区别。首先，英文文本挖掘预处理一般可以不做分词（特殊需求除外），而中文预处理分词是必不可少的一步。第二点，大部分英文文        阅读全文\n",
      " posted @ 2017-04-24 15:12\n",
      "刘建平Pinard\n",
      "阅读(21283)\n",
      "评论(11)\n",
      "推荐(8)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(21283) 评论(11) 推荐(8)\n",
      "\n",
      "        中文文本挖掘预处理流程总结\n",
      "     \n",
      "摘要：在对文本做数据分析时，我们一大半的时间都会花在文本预处理上，而中文和英文的预处理流程稍有不同，本文就对中文文本挖掘的预处理流程做一个总结。 1. 中文文本挖掘预处理特点 首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。 首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接        阅读全文\n",
      " posted @ 2017-04-21 16:58\n",
      "刘建平Pinard\n",
      "阅读(47705)\n",
      "评论(74)\n",
      "推荐(17)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(47705) 评论(74) 推荐(17)\n",
      "\n",
      "        文本挖掘预处理之TF-IDF\n",
      "     \n",
      "摘要：在文本挖掘预处理之向量化与Hash Trick中我们讲到在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。 1. 文本向量化特征的不足 在将文本分词并向量化后，我们可以得到词汇表中每个词在各        阅读全文\n",
      " posted @ 2017-04-11 14:58\n",
      "刘建平Pinard\n",
      "阅读(63084)\n",
      "评论(39)\n",
      "推荐(17)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(63084) 评论(39) 推荐(17)\n",
      "\n",
      "        文本挖掘预处理之向量化与Hash Trick\n",
      "     \n",
      "摘要：在文本挖掘的分词原理中，我们讲到了文本挖掘的预处理的关键一步：“分词”，而在做了分词后，如果我们是做文本分类聚类，则后面关键的特征预处理步骤有向量化或向量化的特例Hash Trick，本文我们就对向量化和特例Hash Trick预处理方法做一个总结。 1. 词袋模型 在讲向量化与Hash Trick        阅读全文\n",
      " posted @ 2017-04-10 14:56\n",
      "刘建平Pinard\n",
      "阅读(22984)\n",
      "评论(36)\n",
      "推荐(7)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(22984) 评论(36) 推荐(7)\n",
      "\n",
      "        文本挖掘的分词原理\n",
      "     \n",
      "摘要：在做文本挖掘的时候，首先要做的预处理就是分词。英文单词天然有空格隔开容易按照空格分词，但是也有时候需要把多个单词做为一个分词，比如一些名词如“New York”，需要做为一个词看待。而中文由于没有空格，分词就是一个需要专门去解决的问题了。无论是英文还是中文，分词的原理都是类似的，本文就对文本挖掘时的        阅读全文\n",
      " posted @ 2017-04-07 14:49\n",
      "刘建平Pinard\n",
      "阅读(30835)\n",
      "评论(63)\n",
      "推荐(15)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(30835) 评论(63) 推荐(15)\n",
      "\n",
      "        MCMC(四)Gibbs采样\n",
      "     \n",
      "摘要：MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(三)MCMC采样和M-H采样中，我们讲到了M-H采样已经可以很好的解决蒙特卡罗方法需要的任意概率分布的样本集的问题。但是M-H采样有两个缺点：一是需要计算接受率，在        阅读全文\n",
      " posted @ 2017-03-30 17:03\n",
      "刘建平Pinard\n",
      "阅读(61681)\n",
      "评论(116)\n",
      "推荐(18)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(61681) 评论(116) 推荐(18)\n",
      "\n",
      "        MCMC(三)MCMC采样和M-H采样\n",
      "     \n",
      "摘要：MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(二)马尔科夫链中我们讲到给定一个概率平稳分布$\\pi$, 很难直接找到对应的马尔科夫链状态转移矩阵$P$。而只要解决这个问题，我们就可以找到一种通用的概率分布采样方        阅读全文\n",
      " posted @ 2017-03-29 15:17\n",
      "刘建平Pinard\n",
      "阅读(69767)\n",
      "评论(187)\n",
      "推荐(22)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(69767) 评论(187) 推荐(22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        MCMC(二)马尔科夫链\n",
      "     \n",
      "摘要：MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(一)蒙特卡罗方法中，我们讲到了如何用蒙特卡罗方法来随机模拟求解一些复杂的连续积分或者离散求和的方法，但是这个方法需要得到对应的概率分布的样本集，而想得到这样的样本集        阅读全文\n",
      " posted @ 2017-03-28 15:05\n",
      "刘建平Pinard\n",
      "阅读(70707)\n",
      "评论(120)\n",
      "推荐(34)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(70707) 评论(120) 推荐(34)\n",
      "\n",
      "        MCMC(一)蒙特卡罗方法\n",
      "     \n",
      "摘要：MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 作为一种随机采样方法，马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，以下简称MCMC）在机器学习,深度学习以及自然语言处理等领域都有广泛的应用，是很多复        阅读全文\n",
      " posted @ 2017-03-27 15:08\n",
      "刘建平Pinard\n",
      "阅读(103468)\n",
      "评论(79)\n",
      "推荐(40)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(103468) 评论(79) 推荐(40)\n",
      "\n",
      "        受限玻尔兹曼机（RBM）原理总结\n",
      "     \n",
      "摘要：在前面我们讲到了深度学习的两类神经网络模型的原理，第一类是前向的神经网络，即DNN和CNN。第二类是有反馈的神经网络，即RNN和LSTM。今天我们就总结下深度学习里的第三类神经网络模型：玻尔兹曼机。主要关注于这类模型中的受限玻尔兹曼机（Restricted Boltzmann Machine，以下简        阅读全文\n",
      " posted @ 2017-03-11 09:50\n",
      "刘建平Pinard\n",
      "阅读(31957)\n",
      "评论(41)\n",
      "推荐(14)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(31957) 评论(41) 推荐(14)\n",
      "\n",
      "        LSTM模型与前向反向传播算法\n",
      "     \n",
      "摘要：在循环神经网络(RNN)模型与前向反向传播算法中，我们总结了对RNN模型做了总结。由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。        阅读全文\n",
      " posted @ 2017-03-08 15:38\n",
      "刘建平Pinard\n",
      "阅读(70694)\n",
      "评论(156)\n",
      "推荐(27)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(70694) 评论(156) 推荐(27)\n",
      "\n",
      "        循环神经网络(RNN)模型与前向反向传播算法\n",
      "     \n",
      "摘要：在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。今天我们就讨论另一类输出和模型间有反馈的神经网络：循环神经网络(Recurrent Neural Networks ，以下简称RNN)，它广泛的用于自然语言处理中的语音识        阅读全文\n",
      " posted @ 2017-03-06 19:57\n",
      "刘建平Pinard\n",
      "阅读(129157)\n",
      "评论(205)\n",
      "推荐(21)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(129157) 评论(205) 推荐(21)\n",
      "\n",
      "        卷积神经网络(CNN)反向传播算法\n",
      "     \n",
      "摘要：在卷积神经网络(CNN)前向传播算法中，我们对CNN的前向传播算法做了总结，基于CNN前向传播算法的基础，我们下面就对CNN的反向传播算法做一个总结。在阅读本文前，建议先研究DNN的反向传播算法：深度神经网络（DNN）反向传播算法(BP) 1. 回顾DNN的反向传播算法 我们首先回顾DNN的反向传播        阅读全文\n",
      " posted @ 2017-03-03 14:13\n",
      "刘建平Pinard\n",
      "阅读(143975)\n",
      "评论(242)\n",
      "推荐(46)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(143975) 评论(242) 推荐(46)\n",
      "\n",
      "        卷积神经网络(CNN)前向传播算法\n",
      "     \n",
      "摘要：在卷积神经网络(CNN)模型结构中，我们对CNN的模型结构做了总结，这里我们就在CNN的模型基础上，看看CNN的前向传播算法是什么样子的。重点会和传统的DNN比较讨论。 1. 回顾CNN的结构 在上一篇里，我们已经讲到了CNN的结构，包括输出层，若干的卷积层+ReLU激活函数，若干的池化层，DNN全        阅读全文\n",
      " posted @ 2017-03-02 12:41\n",
      "刘建平Pinard\n",
      "阅读(50608)\n",
      "评论(49)\n",
      "推荐(14)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(50608) 评论(49) 推荐(14)\n",
      "\n",
      "        卷积神经网络(CNN)模型结构\n",
      "     \n",
      "摘要：在前面我们讲述了DNN的模型与前向反向传播算法。而在DNN大类中，卷积神经网络(Convolutional Neural Networks，以下简称CNN)是最为成功的DNN特例之一。CNN广泛的应用于图像识别，当然现在也应用于NLP等其他领域，本文我们就对CNN的模型结构做一个总结。 在学习CNN        阅读全文\n",
      " posted @ 2017-03-01 14:31\n",
      "刘建平Pinard\n",
      "阅读(134841)\n",
      "评论(66)\n",
      "推荐(27)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(134841) 评论(66) 推荐(27)\n",
      "\n",
      "        深度神经网络（DNN）的正则化\n",
      "     \n",
      "摘要：和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，这里我们就对DNN的正则化方法做一个总结。 1. DNN的L1&L2正则化 想到正则化，我们首先想到的就是L1正则化和L2正则化。L1正则化和L2正则化原理类似，这里重点讲述DNN的L2正则化。 而DNN的L2正则化通常的做法是只针        阅读全文\n",
      " posted @ 2017-02-27 14:20\n",
      "刘建平Pinard\n",
      "阅读(30750)\n",
      "评论(40)\n",
      "推荐(13)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(30750) 评论(40) 推荐(13)\n",
      "\n",
      "        深度神经网络（DNN）损失函数和激活函数的选择\n",
      "     \n",
      "摘要：在深度神经网络（DNN）反向传播算法(BP)中，我们对DNN的前向反向传播算法的使用做了总结。里面使用的损失函数是均方差，而激活函数是Sigmoid。实际上DNN可以使用的损失函数和激活函数不少。这些损失函数和激活函数如何选择呢？下面我们就对DNN损失函数和激活函数的选择做一个总结。 1. 均方差损        阅读全文\n",
      " posted @ 2017-02-24 14:50\n",
      "刘建平Pinard\n",
      "阅读(66411)\n",
      "评论(156)\n",
      "推荐(12)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(66411) 评论(156) 推荐(12)\n",
      "\n",
      "        深度神经网络（DNN）反向传播算法(BP)\n",
      "     \n",
      "摘要：在深度神经网络（DNN）模型与前向传播算法中，我们对DNN的模型和前向传播算法做了总结，这里我们更进一步，对DNN的反向传播算法（Back Propagation，BP）做一个总结。 1. DNN反向传播算法要解决的问题 在了解DNN的反向传播算法前，我们先要知道DNN反向传播算法要解决的问题，也就        阅读全文\n",
      " posted @ 2017-02-21 12:36\n",
      "刘建平Pinard\n",
      "阅读(92784)\n",
      "评论(146)\n",
      "推荐(28)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(92784) 评论(146) 推荐(28)\n",
      "\n",
      "        深度神经网络（DNN）模型与前向传播算法\n",
      "     \n",
      "摘要：深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。 1. 从感知机到神经网络 在感知机原理小结中，我们介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:        阅读全文\n",
      " posted @ 2017-02-20 15:08\n",
      "刘建平Pinard\n",
      "阅读(173867)\n",
      "评论(41)\n",
      "推荐(37)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(173867) 评论(41) 推荐(37)\n",
      "\n",
      "        分解机(Factorization Machines)推荐算法原理\n",
      "     \n",
      "摘要：对于分解机(Factorization Machines，FM)推荐算法原理，本来想自己单独写一篇的。但是看到peghoty写的FM不光简单易懂，而且排版也非常好，因此转载过来，自己就不再单独写FM了。 Pinard注：上面最后一句话应该是\"而$g_{\\theta}(x)$则利用$\\widehat{        阅读全文\n",
      " posted @ 2017-02-06 14:06\n",
      "刘建平Pinard\n",
      "阅读(42247)\n",
      "评论(64)\n",
      "推荐(8)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(42247) 评论(64) 推荐(8)\n",
      "\n",
      "        用Spark学习矩阵分解推荐算法\n",
      "     \n",
      "摘要：在矩阵分解在协同过滤推荐算法中的应用中，我们对矩阵分解在推荐算法中的应用原理做了总结，这里我们就从实践的角度来用Spark学习矩阵分解推荐算法。 1. Spark推荐算法概述 在Spark MLlib中，推荐算法这块只实现了基于矩阵分解的协同过滤推荐算法。而基于的算法是FunkSVD算法，即将m个用        阅读全文\n",
      " posted @ 2017-02-04 15:55\n",
      "刘建平Pinard\n",
      "阅读(18360)\n",
      "评论(44)\n",
      "推荐(6)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(18360) 评论(44) 推荐(6)\n",
      "\n",
      "        SimRank协同过滤推荐算法\n",
      "     \n",
      "摘要：在协同过滤推荐算法总结中，我们讲到了用图模型做协同过滤的方法，包括SimRank系列算法和马尔科夫链系列算法。现在我们就对SimRank算法在推荐系统的应用做一个总结。 1. SimRank推荐算法的图论基础 SimRank是基于图论的，如果用于推荐算法，则它假设用户和物品在空间中形成了一张图。而这        阅读全文\n",
      " posted @ 2017-02-03 15:56\n",
      "刘建平Pinard\n",
      "阅读(15715)\n",
      "评论(36)\n",
      "推荐(4)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(15715) 评论(36) 推荐(4)\n",
      "\n",
      "        矩阵分解在协同过滤推荐算法中的应用\n",
      "     \n",
      "摘要：在协同过滤推荐算法总结中，我们讲到了用矩阵分解做协同过滤是广泛使用的方法，这里就对矩阵分解在协同过滤推荐算法中的应用做一个总结。(过年前最后一篇！祝大家新年快乐！明年的目标是写120篇机器学习，深度学习和NLP相关的文章) 1. 矩阵分解用于推荐算法要解决的问题 在推荐系统中，我们常常遇到的问题是这        阅读全文\n",
      " posted @ 2017-01-26 12:28\n",
      "刘建平Pinard\n",
      "阅读(37559)\n",
      "评论(79)\n",
      "推荐(14)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(37559) 评论(79) 推荐(14)\n",
      "\n",
      "        协同过滤推荐算法总结\n",
      "     \n",
      "摘要：推荐算法具有非常多的应用场景和商业价值，因此对推荐算法值得好好研究。推荐算法种类很多，但是目前应用最广泛的应该是协同过滤类别的推荐算法，本文就对协同过滤类别的推荐算法做一个概括总结，后续也会对一些典型的协同过滤推荐算法做原理总结。 1. 推荐算法概述 推荐算法是非常古老的，在机器学习还没有兴起的时候        阅读全文\n",
      " posted @ 2017-01-25 15:12\n",
      "刘建平Pinard\n",
      "阅读(68140)\n",
      "评论(56)\n",
      "推荐(36)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(68140) 评论(56) 推荐(36)\n",
      "\n",
      "        用Spark学习FP Tree算法和PrefixSpan算法\n",
      "     \n",
      "摘要：在FP Tree算法原理总结和PrefixSpan算法原理总结中，我们对FP Tree和PrefixSpan这两种关联算法的原理做了总结，这里就从实践的角度介绍如何使用这两个算法。由于scikit-learn中没有关联算法的类库，而Spark MLlib有，本文的使用以Spark MLlib作为使用        阅读全文\n",
      " posted @ 2017-01-22 14:24\n",
      "刘建平Pinard\n",
      "阅读(10768)\n",
      "评论(24)\n",
      "推荐(4)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(10768) 评论(24) 推荐(4)\n",
      "\n",
      "        PrefixSpan算法原理总结\n",
      "     \n",
      "摘要：前面我们讲到频繁项集挖掘的关联算法Apriori和FP Tree。这两个算法都是挖掘频繁项集的。而今天我们要介绍的PrefixSpan算法也是关联算法，但是它是挖掘频繁序列模式的，因此要解决的问题目标稍有不同。 1. 项集数据和序列数据 首先我们看看项集数据和序列数据有什么不同，如下图所示。 左边的        阅读全文\n",
      " posted @ 2017-01-20 23:13\n",
      "刘建平Pinard\n",
      "阅读(25390)\n",
      "评论(31)\n",
      "推荐(4)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(25390) 评论(31) 推荐(4)\n",
      "\n",
      "        FP Tree算法原理总结\n",
      "     \n",
      "摘要：在Apriori算法原理总结中，我们对Apriori算法的原理做了总结。作为一个挖掘频繁项集的算法，Apriori算法需要多次扫描数据，I/O是很大的瓶颈。为了解决这个问题，FP Tree算法（也称FP Growth算法）采用了一些技巧，无论多少数据，只需要扫描两次数据集，因此提高了算法运行的效率。        阅读全文\n",
      " posted @ 2017-01-19 21:19\n",
      "刘建平Pinard\n",
      "阅读(41086)\n",
      "评论(53)\n",
      "推荐(28)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(41086) 评论(53) 推荐(28)\n",
      "\n",
      "        Apriori算法原理总结\n",
      "     \n",
      "摘要：Apriori算法是常用的用于挖掘出数据关联规则的算法，它用来找出数据值中频繁出现的数据集合，找出这些集合的模式有助于我们做一些决策。比如在常见的超市购物数据集，或者电商的网购数据集中，如果我们找到了频繁出现的数据集，那么对于超市，我们可以优化产品的位置摆放，对于电商，我们可以优化商品所在的仓库位置        阅读全文\n",
      " posted @ 2017-01-17 17:05\n",
      "刘建平Pinard\n",
      "阅读(78759)\n",
      "评论(39)\n",
      "推荐(22)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(78759) 评论(39) 推荐(22)\n",
      "\n",
      "        典型关联分析(CCA)原理总结\n",
      "     \n",
      "摘要：典型关联分析(Canonical Correlation Analysis，以下简称CCA)是最常用的挖掘数据关联关系的算法之一。比如我们拿到两组数据，第一组是人身高和体重的数据，第二组是对应的跑步能力和跳远能力的数据。那么我们能不能说这两组数据是相关的呢？CCA可以帮助我们分析这个问题。 1. C        阅读全文\n",
      " posted @ 2017-01-16 17:19\n",
      "刘建平Pinard\n",
      "阅读(29925)\n",
      "评论(42)\n",
      "推荐(13)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(29925) 评论(42) 推荐(13)\n",
      "\n",
      "        用scikit-learn研究局部线性嵌入(LLE)\n",
      "     \n",
      "摘要：在局部线性嵌入(LLE)原理总结中，我们对流形学习中的局部线性嵌入(LLE)算法做了原理总结。这里我们就对scikit-learn中流形学习的一些算法做一个介绍，并着重对其中LLE算法的使用方法做一个实践上的总结。 1. scikit-learn流形学习库概述 在scikit-learn中，流形学习        阅读全文\n",
      " posted @ 2017-01-11 16:32\n",
      "刘建平Pinard\n",
      "阅读(7494)\n",
      "评论(0)\n",
      "推荐(3)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(7494) 评论(0) 推荐(3)\n",
      "\n",
      "        局部线性嵌入(LLE)原理总结\n",
      "     \n",
      "摘要：局部线性嵌入(Locally Linear Embedding，以下简称LLE)也是非常重要的降维方法。和传统的PCA，LDA等关注样本方差的降维方法相比，LLE关注于降维时保持样本局部的线性特征，由于LLE在降维时保持了样本的局部特征，它广泛的用于图像图像识别，高维数据可视化等领域。下面我们就对L        阅读全文\n",
      " posted @ 2017-01-10 12:34\n",
      "刘建平Pinard\n",
      "阅读(44168)\n",
      "评论(85)\n",
      "推荐(11)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(44168) 评论(85) 推荐(11)\n",
      "\n",
      "        奇异值分解(SVD)原理与在降维中的应用\n",
      "     \n",
      "摘要：奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SV        阅读全文\n",
      " posted @ 2017-01-05 15:44\n",
      "刘建平Pinard\n",
      "阅读(207797)\n",
      "评论(121)\n",
      "推荐(95)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(207797) 评论(121) 推荐(95)\n",
      "\n",
      "        用scikit-learn进行LDA降维\n",
      "     \n",
      "摘要：在线性判别分析LDA原理总结中，我们对LDA降维的原理做了总结，这里我们就对scikit-learn中LDA的降维使用做一个总结。 1. 对scikit-learn中LDA类概述 在scikit-learn中， LDA类是sklearn.discriminant_analysis.LinearDis        阅读全文\n",
      " posted @ 2017-01-04 17:04\n",
      "刘建平Pinard\n",
      "阅读(30628)\n",
      "评论(29)\n",
      "推荐(7)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(30628) 评论(29) 推荐(7)\n",
      "\n",
      "        线性判别分析LDA原理总结\n",
      "     \n",
      "摘要：在主成分分析（PCA）原理总结中，我们对降维算法PCA做了总结。这里我们就对另外一种经典的降维方法线性判别分析（Linear Discriminant Analysis, 以下简称LDA）做一个总结。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了        阅读全文\n",
      " posted @ 2017-01-03 16:47\n",
      "刘建平Pinard\n",
      "阅读(205003)\n",
      "评论(198)\n",
      "推荐(44)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(205003) 评论(198) 推荐(44)\n",
      "\n",
      "        用scikit-learn学习主成分分析(PCA)\n",
      "     \n",
      "摘要：在主成分分析（PCA）原理总结中，我们对主成分分析(以下简称PCA)的原理做了总结，下面我们就总结下如何使用scikit-learn工具来进行PCA降维。 1. scikit-learn PCA类介绍 在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。        阅读全文\n",
      " posted @ 2017-01-02 20:55\n",
      "刘建平Pinard\n",
      "阅读(108377)\n",
      "评论(71)\n",
      "推荐(14)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(108377) 评论(71) 推荐(14)\n",
      "\n",
      "        主成分分析（PCA）原理总结\n",
      "     \n",
      "摘要：主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。一般我们提到降维最容易想到的算法就是PCA，下面我们就对PCA的原理做一个总结。 1. PCA的思想 PCA顾名思义，就是找出数据里最        阅读全文\n",
      " posted @ 2016-12-31 21:07\n",
      "刘建平Pinard\n",
      "阅读(149508)\n",
      "评论(221)\n",
      "推荐(42)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(149508) 评论(221) 推荐(42)\n",
      "\n",
      "        用scikit-learn学习谱聚类\n",
      "     \n",
      "摘要：在谱聚类（spectral clustering）原理总结中，我们对谱聚类的原理做了总结。这里我们就对scikit-learn中谱聚类的使用做一个总结。 1. scikit-learn谱聚类概述 在scikit-learn的类库中，sklearn.cluster.SpectralClustering        阅读全文\n",
      " posted @ 2016-12-30 17:16\n",
      "刘建平Pinard\n",
      "阅读(28983)\n",
      "评论(44)\n",
      "推荐(5)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(28983) 评论(44) 推荐(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        谱聚类（spectral clustering）原理总结\n",
      "     \n",
      "摘要：谱聚类（spectral clustering）是广泛使用的聚类算法，比起传统的K-Means算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多，更加难能可贵的是实现起来也不复杂。在处理实际的聚类问题时，个人认为谱聚类是应该首先考虑的几种算法之一。下面我们就对谱聚类的算法        阅读全文\n",
      " posted @ 2016-12-29 11:11\n",
      "刘建平Pinard\n",
      "阅读(195285)\n",
      "评论(267)\n",
      "推荐(48)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(195285) 评论(267) 推荐(48)\n",
      "\n",
      "        用scikit-learn学习DBSCAN聚类\n",
      "     \n",
      "摘要：在DBSCAN密度聚类算法中，我们对DBSCAN聚类算法的原理做了总结，本文就对如何用scikit-learn来学习DBSCAN聚类做一个总结，重点讲述参数的意义和需要调参的参数。 1. scikit-learn中的DBSCAN类 在scikit-learn中，DBSCAN算法类为sklearn.c        阅读全文\n",
      " posted @ 2016-12-24 18:54\n",
      "刘建平Pinard\n",
      "阅读(83593)\n",
      "评论(75)\n",
      "推荐(9)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(83593) 评论(75) 推荐(9)\n",
      "\n",
      "        DBSCAN密度聚类算法\n",
      "     \n",
      "摘要：DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用        阅读全文\n",
      " posted @ 2016-12-22 16:32\n",
      "刘建平Pinard\n",
      "阅读(162325)\n",
      "评论(73)\n",
      "推荐(30)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(162325) 评论(73) 推荐(30)\n",
      "\n",
      "        用scikit-learn学习BIRCH聚类\n",
      "     \n",
      "摘要：在BIRCH聚类算法原理中，我们对BIRCH聚类算法的原理做了总结，本文就对scikit-learn中BIRCH算法的使用做一个总结。 1. scikit-learn之BIRCH类 在scikit-learn中，BIRCH类实现了原理篇里讲到的基于特征树CF Tree的聚类。因此要使用BIRCH来聚        阅读全文\n",
      " posted @ 2016-12-19 22:00\n",
      "刘建平Pinard\n",
      "阅读(17884)\n",
      "评论(52)\n",
      "推荐(4)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(17884) 评论(52) 推荐(4)\n",
      "\n",
      "        BIRCH聚类算法原理\n",
      "     \n",
      "摘要：在K-Means聚类算法原理中，我们讲到了K-Means和Mini Batch K-Means的聚类原理。这里我们再来看看另外一种常见的聚类算法BIRCH。BIRCH算法比较适合于数据量大，类别数K也比较多的情况。它运行速度很快，只需要单遍扫描数据集就能进行聚类，当然需要用到一些技巧，下面我们就对B        阅读全文\n",
      " posted @ 2016-12-14 17:13\n",
      "刘建平Pinard\n",
      "阅读(52251)\n",
      "评论(72)\n",
      "推荐(18)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(52251) 评论(72) 推荐(18)\n",
      "\n",
      "        用scikit-learn学习K-Means聚类\n",
      "     \n",
      "摘要：在K-Means聚类算法原理中，我们对K-Means的原理做了总结，本文我们就来讨论用scikit-learn来学习K-Means聚类。重点讲述如何选择合适的k值。 1. K-Means类概述 在scikit-learn中，包括两个K-Means的算法，一个是传统的K-Means算法，对应的类是KM        阅读全文\n",
      " posted @ 2016-12-13 15:50\n",
      "刘建平Pinard\n",
      "阅读(87899)\n",
      "评论(64)\n",
      "推荐(12)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(87899) 评论(64) 推荐(12)\n",
      "\n",
      "        K-Means聚类算法原理\n",
      "     \n",
      "摘要：K-Means算法是无监督的聚类算法，它实现起来比较简单，聚类效果也不错，因此应用很广泛。K-Means算法有大量的变体，本文就从最传统的K-Means算法讲起，在其基础上讲述K-Means的优化变体方法。包括初始化优化K-Means++, 距离计算优化elkan K-Means算法和大数据情况下的        阅读全文\n",
      " posted @ 2016-12-12 16:57\n",
      "刘建平Pinard\n",
      "阅读(178534)\n",
      "评论(70)\n",
      "推荐(21)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(178534) 评论(70) 推荐(21)\n",
      "\n",
      "        scikit-learn随机森林调参小结\n",
      "     \n",
      "摘要：在Bagging与随机森林算法原理小结中，我们对随机森林(Random Forest, 以下简称RF）的原理做了总结。本文就从实践的角度对RF做一个总结。重点讲述scikit-learn中RF的调参注意事项，以及和GBDT调参的异同点。 1. scikit-learn随机森林类库概述 在scikit        阅读全文\n",
      " posted @ 2016-12-11 21:23\n",
      "刘建平Pinard\n",
      "阅读(112570)\n",
      "评论(153)\n",
      "推荐(26)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(112570) 评论(153) 推荐(26)\n",
      "\n",
      "        Bagging与随机森林算法原理小结\n",
      "     \n",
      "摘要：在集成学习原理小结中，我们讲到了集成学习有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。本文就对集成学习中Bagging与随机森林算法做一个总结。 随机森林是集成学习中可以和梯度提升树GB        阅读全文\n",
      " posted @ 2016-12-10 20:38\n",
      "刘建平Pinard\n",
      "阅读(99341)\n",
      "评论(157)\n",
      "推荐(28)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(99341) 评论(157) 推荐(28)\n",
      "\n",
      "        scikit-learn 梯度提升树(GBDT)调参小结\n",
      "     \n",
      "摘要：在梯度提升树(GBDT)原理小结中，我们对GBDT的原理做了总结，本文我们就从scikit-learn里GBDT的类库使用方法作一个总结，主要会关注调参中的一些要点。 1. scikit-learn GBDT类库概述 在sacikit-learn中，GradientBoostingClassifie        阅读全文\n",
      " posted @ 2016-12-09 17:17\n",
      "刘建平Pinard\n",
      "阅读(88410)\n",
      "评论(95)\n",
      "推荐(21)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(88410) 评论(95) 推荐(21)\n",
      "\n",
      "        梯度提升树(GBDT)原理小结\n",
      "     \n",
      "摘要：在集成学习之Adaboost算法原理小结中，我们对Boosting家族的Adaboost算法做了总结，本文就对Boosting家族中另一个重要的算法梯度提升树(Gradient Boosting Decison Tree, 以下简称GBDT)做一个总结。GBDT有很多简称，有GBT（Gradient        阅读全文\n",
      " posted @ 2016-12-07 19:59\n",
      "刘建平Pinard\n",
      "阅读(262906)\n",
      "评论(561)\n",
      "推荐(54)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(262906) 评论(561) 推荐(54)\n",
      "\n",
      "        scikit-learn Adaboost类库使用小结\n",
      "     \n",
      "摘要：在集成学习之Adaboost算法原理小结中，我们对Adaboost的算法原理做了一个总结。这里我们就从实用的角度对scikit-learn中Adaboost类库的使用做一个小结，重点对调参的注意事项做一个总结。 1. Adaboost类库概述 scikit-learn中Adaboost类库比较直接，        阅读全文\n",
      " posted @ 2016-12-06 19:41\n",
      "刘建平Pinard\n",
      "阅读(69164)\n",
      "评论(100)\n",
      "推荐(26)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(69164) 评论(100) 推荐(26)\n",
      "\n",
      "        集成学习之Adaboost算法原理小结\n",
      "     \n",
      "摘要：在集成学习原理小结中，我们讲到了集成学习按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，另一类是个体学习器之间不存在强依赖关系。前者的代表算法就是是boosting系列算法。在boosting系列算法中， Adaboost是最著名的算法之一。Adaboost既可        阅读全文\n",
      " posted @ 2016-12-05 22:26\n",
      "刘建平Pinard\n",
      "阅读(99845)\n",
      "评论(332)\n",
      "推荐(48)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(99845) 评论(332) 推荐(48)\n",
      "\n",
      "        集成学习原理小结\n",
      "     \n",
      "摘要：集成学习(ensemble learning)可以说是现在非常火爆的机器学习方法了。它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。也就是我们常说的“博采众长”。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等等，可以说所有的机器学习领域都        阅读全文\n",
      " posted @ 2016-12-04 20:48\n",
      "刘建平Pinard\n",
      "阅读(78042)\n",
      "评论(55)\n",
      "推荐(48)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(78042) 评论(55) 推荐(48)\n",
      "\n",
      "        支持向量机高斯核调参小结\n",
      "     \n",
      "摘要：在支持向量机(以下简称SVM)的核函数中，高斯核(以下简称RBF)是最常用的，从理论上讲， RBF一定不比线性核函数差，但是在实际应用中，却面临着几个重要的超参数的调优问题。如果调的不好，可能比线性核函数还要差。所以我们实际应用中，能用线性核函数得到较好效果的都会选择线性核函数。如果线性核不好，我们        阅读全文\n",
      " posted @ 2016-12-02 21:59\n",
      "刘建平Pinard\n",
      "阅读(28981)\n",
      "评论(58)\n",
      "推荐(15)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(28981) 评论(58) 推荐(15)\n",
      "\n",
      "        scikit-learn 支持向量机算法库使用小结\n",
      "     \n",
      "摘要：之前通过一个系列对支持向量机(以下简称SVM)算法的原理做了一个总结，本文从实践的角度对scikit-learn SVM算法库的使用做一个小结。scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分。 1. scikit-learn SVM        阅读全文\n",
      " posted @ 2016-11-30 16:47\n",
      "刘建平Pinard\n",
      "阅读(38206)\n",
      "评论(39)\n",
      "推荐(8)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(38206) 评论(39) 推荐(8)\n",
      "\n",
      "        支持向量机原理(五)线性支持回归\n",
      "     \n",
      "摘要：支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在前四篇里面我们讲到了SVM的线性分类和非线性分类，以及在分类时用到的算法。这些都关注        阅读全文\n",
      " posted @ 2016-11-29 16:53\n",
      "刘建平Pinard\n",
      "阅读(29399)\n",
      "评论(41)\n",
      "推荐(10)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(29399) 评论(41) 推荐(10)\n",
      "\n",
      "        支持向量机原理(三)线性不可分支持向量机与核函数\n",
      "     \n",
      "摘要：支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在前面两篇我们讲到了线性可分SVM的硬间隔最大化和软间隔最大化的算法，它们对线性可分的        阅读全文\n",
      " posted @ 2016-11-26 11:33\n",
      "刘建平Pinard\n",
      "阅读(26704)\n",
      "评论(40)\n",
      "推荐(11)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(26704) 评论(40) 推荐(11)\n",
      "\n",
      "        支持向量机原理(二) 线性支持向量机的软间隔最大化模型\n",
      "     \n",
      "摘要：支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在支持向量机原理(一) 线性支持向量机中，我们对线性可分SVM的模型和损失函数优化做了        阅读全文\n",
      " posted @ 2016-11-25 14:21\n",
      "刘建平Pinard\n",
      "阅读(32807)\n",
      "评论(62)\n",
      "推荐(14)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(32807) 评论(62) 推荐(14)\n",
      "\n",
      "        支持向量机原理(一) 线性支持向量机\n",
      "     \n",
      "摘要：支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 支持向量机(Support Vecor Machine,以下简称SVM)虽然诞生只有短        阅读全文\n",
      " posted @ 2016-11-24 21:33\n",
      "刘建平Pinard\n",
      "阅读(84055)\n",
      "评论(92)\n",
      "推荐(39)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(84055) 评论(92) 推荐(39)\n",
      "\n",
      "        最大熵模型原理小结\n",
      "     \n",
      "摘要：最大熵模型(maximum entropy model， MaxEnt)也是很典型的分类算法了，它和逻辑回归类似，都是属于对数线性分类模型。在损失函数优化的过程中，使用了和支持向量机类似的凸优化技术。而对熵的使用，让我们想起了决策树算法中的ID3和C4.5算法。理解了最大熵模型，对逻辑回归，支持向量        阅读全文\n",
      " posted @ 2016-11-23 20:33\n",
      "刘建平Pinard\n",
      "阅读(36022)\n",
      "评论(84)\n",
      "推荐(9)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(36022) 评论(84) 推荐(9)\n",
      "\n",
      "        scikit-learn 朴素贝叶斯类库使用小结\n",
      "     \n",
      "摘要：之前在朴素贝叶斯算法原理小结这篇文章中，对朴素贝叶斯分类算法的原理做了一个总结。这里我们就从实战的角度来看朴素贝叶斯类库。重点讲述scikit-learn 朴素贝叶斯类库的使用要点和参数选择。 1. scikit-learn 朴素贝叶斯类库概述 朴素贝叶斯是一类比较简单的算法，scikit-lear        阅读全文\n",
      " posted @ 2016-11-17 17:03\n",
      "刘建平Pinard\n",
      "阅读(40737)\n",
      "评论(28)\n",
      "推荐(16)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(40737) 评论(28) 推荐(16)\n",
      "\n",
      "        朴素贝叶斯算法原理小结\n",
      "     \n",
      "摘要：在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数$Y=f(X)$,要么是条件分布$P(Y|X)$。但是朴素贝叶斯却是生成方法，也就是直        阅读全文\n",
      " posted @ 2016-11-16 17:25\n",
      "刘建平Pinard\n",
      "阅读(93639)\n",
      "评论(107)\n",
      "推荐(33)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(93639) 评论(107) 推荐(33)\n",
      "\n",
      "        scikit-learn K近邻法类库使用小结\n",
      "     \n",
      "摘要：在K近邻法(KNN)原理小结这篇文章，我们讨论了KNN的原理和优缺点，这里我们就从实践出发，对scikit-learn 中KNN相关的类库使用做一个小结。主要关注于类库调参时的一个经验总结。 1. scikit-learn 中KNN相关的类库概述 在scikit-learn 中，与近邻法这一大类相关        阅读全文\n",
      " posted @ 2016-11-15 16:29\n",
      "刘建平Pinard\n",
      "阅读(29876)\n",
      "评论(7)\n",
      "推荐(7)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(29876) 评论(7) 推荐(7)\n",
      "\n",
      "        K近邻法(KNN)原理小结\n",
      "     \n",
      "摘要：K近邻法(k-nearest neighbors,KNN)是一种很基本的机器学习方法了，在我们平常的生活中也会不自主的应用。比如，我们判断一个人的人品，只需要观察他来往最密切的几个人的人品好坏就可以得出了。这里就运用了KNN的思想。KNN方法既可以做分类，也可以做回归，这点和决策树算法相同。 KNN        阅读全文\n",
      " posted @ 2016-11-14 20:13\n",
      "刘建平Pinard\n",
      "阅读(62113)\n",
      "评论(82)\n",
      "推荐(22)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(62113) 评论(82) 推荐(22)\n",
      "\n",
      "        scikit-learn决策树算法类库使用小结\n",
      "     \n",
      "摘要：之前对决策树的算法原理做了总结，包括决策树算法原理(上)和决策树算法原理(下)。今天就从实践的角度来介绍决策树算法，主要是讲解使用scikit-learn来跑决策树算法，结果的可视化以及一些参数调参的关键点。 1. scikit-learn决策树算法类库介绍 scikit-learn决策树算法类库内        阅读全文\n",
      " posted @ 2016-11-12 14:28\n",
      "刘建平Pinard\n",
      "阅读(118207)\n",
      "评论(120)\n",
      "推荐(23)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(118207) 评论(120) 推荐(23)\n",
      "\n",
      "        决策树算法原理(下)\n",
      "     \n",
      "摘要：在决策树算法原理(上)这篇里，我们讲到了决策树里ID3算法，和ID3算法的改进版C4.5算法。对于C4.5算法，我们也提到了它的不足，比如模型是用较为复杂的熵来度量，使用了相对较为复杂的多叉树，只能处理分类不能处理回归等。对于这些问题， CART算法大部分做了改进。CART算法也就是我们下面的重点了        阅读全文\n",
      " posted @ 2016-11-11 16:10\n",
      "刘建平Pinard\n",
      "阅读(89719)\n",
      "评论(321)\n",
      "推荐(33)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(89719) 评论(321) 推荐(33)\n",
      "\n",
      "        决策树算法原理(上)\n",
      "     \n",
      "摘要：决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。本文就对决策树算法原理做一个总结，上篇对ID3， C4.5的算法思想做了总结，下篇重点对CART算法做一个详细的介绍。选择CART做重点介绍的原因是scikit-learn        阅读全文\n",
      " posted @ 2016-11-10 15:54\n",
      "刘建平Pinard\n",
      "阅读(96848)\n",
      "评论(133)\n",
      "推荐(34)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(96848) 评论(133) 推荐(34)\n",
      "\n",
      "        机器学习算法的随机数据生成\n",
      "     \n",
      "摘要：在学习机器学习算法的过程中，我们经常需要数据来验证算法，调试参数。但是找到一组十分合适某种特定算法类型的数据样本却不那么容易。还好numpy, scikit-learn都提供了随机数据生成的功能，我们可以自己生成适合某一种模型的数据，用随机数据来做清洗，归一化，转换，然后选择模型与算法做拟合和预测。        阅读全文\n",
      " posted @ 2016-11-09 22:03\n",
      "刘建平Pinard\n",
      "阅读(25264)\n",
      "评论(11)\n",
      "推荐(14)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(25264) 评论(11) 推荐(14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        感知机原理小结\n",
      "     \n",
      "摘要：感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。这里对感知机的原理做一个小结。         阅读全文\n",
      " posted @ 2016-11-08 16:23\n",
      "刘建平Pinard\n",
      "阅读(35879)\n",
      "评论(96)\n",
      "推荐(25)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(35879) 评论(96) 推荐(25)\n",
      "\n",
      "        日志和告警数据挖掘经验谈\n",
      "     \n",
      "摘要：最近参与了了一个日志和告警的数据挖掘项目，里面用到的一些思路在这里和大家做一个分享。 项目的需求是收集的客户系统一个月300G左右的的日志和告警数据做一个整理，主要是归类(Grouping)和关联(Correlation)，从而得到告警和日志的一些统计关系，这些统计结果可以给一线支持人员参考。 得到        阅读全文\n",
      " posted @ 2016-11-07 17:23\n",
      "刘建平Pinard\n",
      "阅读(14607)\n",
      "评论(17)\n",
      "推荐(4)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(14607) 评论(17) 推荐(4)\n",
      "\n",
      "        scikit-learn 逻辑回归类库使用小结\n",
      "     \n",
      "摘要：之前在逻辑回归原理小结这篇文章中，对逻辑回归的原理做了小结。这里接着对scikit-learn中逻辑回归类库的我的使用经验做一个总结。重点讲述调参中要注意的事项。 1. 概述 在scikit-learn中，与逻辑回归有关的主要是这3个类。LogisticRegression， LogisticReg        阅读全文\n",
      " posted @ 2016-11-06 19:41\n",
      "刘建平Pinard\n",
      "阅读(40275)\n",
      "评论(62)\n",
      "推荐(11)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(40275) 评论(62) 推荐(11)\n",
      "\n",
      "        逻辑回归原理小结\n",
      "     \n",
      "摘要：逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。虽然它名字里面有“回归”两个字，却不是一个回归算法。那为什么有“回归”这个误导性的词呢？个人认为，虽然逻辑回归是分类模型，但是它的原理里面却残留着回归模型的影子，本文对逻辑回归原理做一个总结。 1. 从线性回归到逻辑回归 我们知道，线性回归的模        阅读全文\n",
      " posted @ 2016-11-04 21:22\n",
      "刘建平Pinard\n",
      "阅读(74726)\n",
      "评论(185)\n",
      "推荐(24)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(74726) 评论(185) 推荐(24)\n",
      "\n",
      "        scikit-learn 线性回归算法库小结\n",
      "     \n",
      "摘要：scikit-learn对于线性回归提供了比较多的类库，这些类库都可以用来做线性回归分析，本文就对这些类库的使用做一个总结，重点讲述这些线性回归算法库的不同和各自的使用场景。 线性回归的目的是要得到输出向量\\(\\mathbf{Y}\\)和输入特征\\(\\mathbf{X}\\)之间的线性关系，求出线性回归        阅读全文\n",
      " posted @ 2016-11-03 23:41\n",
      "刘建平Pinard\n",
      "阅读(25870)\n",
      "评论(27)\n",
      "推荐(11)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(25870) 评论(27) 推荐(11)\n",
      "\n",
      "        用scikit-learn和pandas学习Ridge回归\n",
      "     \n",
      "摘要：本文将用一个例子来讲述怎么用scikit-learn和pandas来学习Ridge回归。 1. Ridge回归的损失函数 在我的另外一遍讲线性回归的文章中，对Ridge回归做了一些介绍，以及什么时候适合用 Ridge回归。如果对什么是Ridge回归还完全不清楚的建议阅读我这篇文章。 线性回归原理小结        阅读全文\n",
      " posted @ 2016-11-02 16:34\n",
      "刘建平Pinard\n",
      "阅读(19518)\n",
      "评论(44)\n",
      "推荐(7)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(19518) 评论(44) 推荐(7)\n",
      "\n",
      "        Lasso回归算法： 坐标轴下降法与最小角回归法小结\n",
      "     \n",
      "摘要：前面的文章对线性回归做了一个小结，文章在这： 线性回归原理小结。里面对线程回归的正则化也做了一个初步的介绍。提到了线程回归的L2正则化-Ridge回归，以及线程回归的L1正则化-Lasso回归。但是对于Lasso回归的解法没有提及，本文是对该文的补充和扩展。以下都用矩阵法表示，如果对于矩阵分析不熟悉        阅读全文\n",
      " posted @ 2016-11-01 17:29\n",
      "刘建平Pinard\n",
      "阅读(52181)\n",
      "评论(65)\n",
      "推荐(20)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(52181) 评论(65) 推荐(20)\n",
      "\n",
      "        用scikit-learn和pandas学习线性回归\n",
      "     \n",
      "摘要：对于想深入了解线性回归的童鞋，这里给出一个完整的例子，详细学完这个例子，对用scikit-learn来运行线性回归，评估模型不会有什么问题了。 1. 获取数据，定义问题 没有数据，当然没法研究机器学习啦。:) 这里我们用UCI大学公开的机器学习数据来跑线性回归。 数据的介绍在这： http://ar        阅读全文\n",
      " posted @ 2016-10-31 17:37\n",
      "刘建平Pinard\n",
      "阅读(76359)\n",
      "评论(68)\n",
      "推荐(17)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(76359) 评论(68) 推荐(17)\n",
      "\n",
      "        scikit-learn 和pandas 基于windows单机机器学习环境的搭建\n",
      "     \n",
      "摘要：很多朋友想学习机器学习，却苦于环境的搭建，这里给出windows上scikit-learn研究开发环境的搭建步骤。 Step 1. Python的安装 python有2.x和3.x的版本之分，但是很多好的机器学习python库都不支持3.x，因此，推荐安装2.7版本的python。当前最新的pyth        阅读全文\n",
      " posted @ 2016-10-30 17:37\n",
      "刘建平Pinard\n",
      "阅读(10854)\n",
      "评论(7)\n",
      "推荐(2)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(10854) 评论(7) 推荐(2)\n",
      "\n",
      "        机器学习研究与开发平台的选择\n",
      "     \n",
      "摘要：目前机器学习可以说是百花齐放阶段，不过如果要学习或者研究机器学习，进而用到生产环境，对平台，开发语言，机器学习库的选择就要费一番脑筋了。这里就我自己的机器学习经验做一个建议，仅供参考。 首先，对于平台选择的第一个问题是，你是要用于生产环境，也就是具体的产品中,还是仅仅是做研究学习用？ 1. 生产环境        阅读全文\n",
      " posted @ 2016-10-28 12:15\n",
      "刘建平Pinard\n",
      "阅读(18594)\n",
      "评论(30)\n",
      "推荐(14)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(18594) 评论(30) 推荐(14)\n",
      "\n",
      "        线性回归原理小结\n",
      "     \n",
      "摘要：线性回归可以说是机器学习中最基本的问题类型了，这里就对线性回归的原理和算法做一个小结。 1. 线性回归的模型函数和损失函数 线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应于n维特征和一个结果输出，如下： \\((x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_        阅读全文\n",
      " posted @ 2016-10-28 11:12\n",
      "刘建平Pinard\n",
      "阅读(40062)\n",
      "评论(101)\n",
      "推荐(10)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(40062) 评论(101) 推荐(10)\n",
      "\n",
      "        精确率与召回率，RoC曲线与PR曲线\n",
      "     \n",
      "摘要：在机器学习的算法评估中，尤其是分类算法评估中，我们经常听到精确率(precision)与召回率(recall)，RoC曲线与PR曲线这些概念，那这些概念到底有什么用处呢？ 首先，我们需要搞清楚几个拗口的概念： 1. TP, FP, TN, FN 听起来还是很费劲，不过我们用一张图就很容易理解了。图如        阅读全文\n",
      " posted @ 2016-10-24 16:22\n",
      "刘建平Pinard\n",
      "阅读(27023)\n",
      "评论(35)\n",
      "推荐(9)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(27023) 评论(35) 推荐(9)\n",
      "\n",
      "        最小二乘法小结\n",
      "     \n",
      "摘要：最小二乘法是用来做函数拟合或者求函数极值的方法。在机器学习，尤其是回归模型中，经常可以看到最小二乘法的身影，这里就对我对最小二乘法的认知做一个小结。 1.最小二乘法的原理与要解决的问题 最小二乘法是由勒让德在19世纪发现的，原理的一般形式很简单，当然发现的过程是非常艰难的。形式如下式：$$目标函数         阅读全文\n",
      " posted @ 2016-10-19 12:31\n",
      "刘建平Pinard\n",
      "阅读(66085)\n",
      "评论(63)\n",
      "推荐(44)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(66085) 评论(63) 推荐(44)\n",
      "\n",
      "        梯度下降（Gradient Descent）小结\n",
      "     \n",
      "摘要：在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。 1. 梯度 在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数        阅读全文\n",
      " posted @ 2016-10-17 22:49\n",
      "刘建平Pinard\n",
      "阅读(361301)\n",
      "评论(204)\n",
      "推荐(116)\n",
      "\n",
      "    编辑\n",
      "\n",
      " 阅读(361301) 评论(204) 推荐(116)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "\n",
    "# 用bs4解析xtml\n",
    "url = 'https://www.cnblogs.com/pinard/default.html?page={}'\n",
    "urls = [url.format(i) for i in range(1,15)]\n",
    "print(len(urls))  # 一共14页的数据\n",
    "\n",
    "'''\n",
    ". 表示 对class属性进行筛选\n",
    "# 表示 对id属性进行筛选\n",
    "'''\n",
    "\n",
    "# 获得每页的标题\n",
    "for url in urls:\n",
    "    response = requests.get(url)                  # 请求\n",
    "    soup = BeautifulSoup(response.text,'lxml')     \n",
    "    blogs = soup.select('.day')  # 一页里面day属性有十个，有十篇博客\n",
    "    for blog in blogs:                        \n",
    "        title = blog.select('.postTitle a span')[0].get_text()  # 点.表示class属性   .postTitle 表示 class = postTitle         \n",
    "        abstract = blog.select('.postCon div')[0].get_text()    # 空格表示下一级\n",
    "        date = blog.select('.postDesc')[0].get_text() \n",
    "        read = blog.select('.post-view-count')[0].get_text() \n",
    "        pinlun = blog.select('.post-comment-count')[0].get_text() \n",
    "        tuijian = blog.select('.post-digg-count')[0].get_text() \n",
    "        \n",
    "        print(title,abstract,date,read,pinlun,tuijian) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
